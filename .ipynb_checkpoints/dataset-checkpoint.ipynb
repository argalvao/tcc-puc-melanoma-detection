{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb4cb705-df20-43c0-9379-91c0511b0b99",
   "metadata": {},
   "source": [
    "Origem dos datasets\n",
    "\n",
    "ISIC:\n",
    "\n",
    "* https://gallery.isic-archive.com/#!/topWithHeader/onlyHeaderTop/gallery?filter=%5B%22mel_class%7Cmelanoma%20in%20situ%22%5D\n",
    "* https://gallery.isic-archive.com/#!/topWithHeader/onlyHeaderTop/gallery?filter=%5B%22diagnosis%7Cangioma%22%2C%22diagnosis%7CAIMP%22%5D&name=\n",
    "\n",
    "Kaggle:\n",
    "\n",
    "* https://www.kaggle.com/datasets/cdeotte/jpeg-melanoma-512x512?resource=download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f3ac14-20cf-446d-82d3-1b231288c61f",
   "metadata": {},
   "source": [
    "Importando as dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cceea70-b309-4dd3-bfbb-50323a99f446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "import shutil\n",
    "import sklearn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3bc3f3-3fe0-440b-9fcd-70874d4f9a19",
   "metadata": {},
   "source": [
    "Redução do volume de dados no dataser Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69892c6e-631b-42c9-8d62-ceef60fedbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduzir_imagens(diretorio, quantidade_desejada):\n",
    "    # Obter lista de arquivos\n",
    "    arquivos = [f for f in os.listdir(diretorio) if os.path.isfile(os.path.join(diretorio, f))]\n",
    "    \n",
    "    # Verificar a quantidade de imagens\n",
    "    if len(arquivos) > quantidade_desejada:\n",
    "        # Selecionar aleatoriamente\n",
    "        arquivos_para_manter = random.sample(arquivos, quantidade_desejada)\n",
    "        arquivos_para_remover = set(arquivos) - set(arquivos_para_manter)\n",
    "        \n",
    "        # Remover as imagens\n",
    "        for arquivo in arquivos_para_remover:\n",
    "            caminho_arquivo = os.path.join(diretorio, arquivo)\n",
    "            os.remove(caminho_arquivo)\n",
    "    else:\n",
    "        print(f\"O diretório {diretorio} já possui {len(arquivos)} ou menos arquivos.\")\n",
    "\n",
    "diretorio_train = \"/home/abel/Downloads/TCC-PUC-Melanoma-Detection/archive/train\"\n",
    "diretorio_test = \"/home/abel/Downloads/TCC-PUC-Melanoma-Detection/archive/test\"\n",
    "\n",
    "# Reduzir imagens nos diretórios\n",
    "reduzir_imagens(diretorio_train, 1326)\n",
    "reduzir_imagens(diretorio_test, 440)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94448700-9b29-43db-ae7b-9924e0e4679a",
   "metadata": {},
   "source": [
    "Unificando os datasets de malanoma (Kaggle e ISIC melanoma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9a555d2-0072-4772-8530-049344dbb2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagens ausentes no dataset Kaggle (train): 31800\n",
      "Imagens ausentes no dataset Kaggle (test): 10542\n",
      "Imagens duplicadas encontradas: 450\n",
      "Total de imagens no novo dataset: 2100\n",
      "Unificação completa. Os arquivos estão salvos em: /home/abel/Downloads/TCC-PUC-Melanoma-Detection/dataset\n"
     ]
    }
   ],
   "source": [
    "# Caminho dos datasets\n",
    "isic_dir = '/home/abel/Downloads/TCC-PUC-Melanoma-Detection/ISIC-images'  # Caminho correto\n",
    "archive_dir = '/home/abel/Downloads/TCC-PUC-Melanoma-Detection/archive'   # Caminho correto\n",
    "output_dir = '/home/abel/Downloads/TCC-PUC-Melanoma-Detection/dataset'  # Diretório de saída\n",
    "\n",
    "# Verificar se os diretórios existem\n",
    "if not os.path.exists(isic_dir):\n",
    "    raise FileNotFoundError(f\"O diretório {isic_dir} não existe. Verifique o caminho.\")\n",
    "if not os.path.exists(archive_dir):\n",
    "    raise FileNotFoundError(f\"O diretório {archive_dir} não existe. Verifique o caminho.\")\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "image_output_dir = os.path.join(output_dir, 'melanoma')\n",
    "os.makedirs(image_output_dir, exist_ok=True)\n",
    "\n",
    "# Unificar imagens do dataset ISIC\n",
    "isic_metadata_path = os.path.join(isic_dir, 'metadata.csv')\n",
    "\n",
    "if not os.path.exists(isic_metadata_path):\n",
    "    raise FileNotFoundError(f\"O arquivo de metadados {isic_metadata_path} não foi encontrado. Verifique o caminho.\")\n",
    "\n",
    "isic_metadata = pd.read_csv(isic_metadata_path)\n",
    "\n",
    "# Verificar imagens ausentes do dataset ISIC\n",
    "imagens_ausentes_isic = isic_metadata[~isic_metadata['isic_id'].apply(lambda x: os.path.exists(os.path.join(isic_dir, f'{x}.jpg')))]\n",
    "if len(imagens_ausentes_isic) > 0:\n",
    "    print(f\"Imagens ausentes no dataset ISIC: {len(imagens_ausentes_isic)}\")\n",
    "\n",
    "# Filtrar metadados para verificar apenas as imagens existentes no diretório\n",
    "isic_metadata = isic_metadata[isic_metadata['isic_id'].apply(lambda x: os.path.exists(os.path.join(isic_dir, f'{x}.jpg')))]\n",
    "\n",
    "# Copiar as imagens para o local do dataset\n",
    "for image_name in isic_metadata['isic_id']:\n",
    "    src_path = os.path.join(isic_dir, f'{image_name}.jpg')\n",
    "    dest_path = os.path.join(image_output_dir, f'{image_name}.jpg')\n",
    "    shutil.copy(src_path, dest_path)\n",
    "\n",
    "isic_metadata['image_path'] = isic_metadata['isic_id'].apply(lambda x: os.path.join(image_output_dir, f'{x}.jpg'))\n",
    "\n",
    "# Unificar imagens do dataset Kaggle\n",
    "archive_train_metadata_path = os.path.join(archive_dir, 'train.csv')\n",
    "archive_test_metadata_path = os.path.join(archive_dir, 'test.csv')\n",
    "\n",
    "archive_train_metadata = pd.read_csv(archive_train_metadata_path)\n",
    "archive_test_metadata = pd.read_csv(archive_test_metadata_path)\n",
    "\n",
    "# Identificar colunas comuns\n",
    "common_columns = list(set(archive_train_metadata.columns) & set(archive_test_metadata.columns))\n",
    "\n",
    "# Manter apenas colunas comuns\n",
    "archive_train_metadata = archive_train_metadata[common_columns]\n",
    "archive_test_metadata = archive_test_metadata[common_columns]\n",
    "\n",
    "# Verificar imagens ausentes no dataset Kaggle (train e test)\n",
    "imagens_ausentes_train = archive_train_metadata[~archive_train_metadata['image_name'].apply(lambda x: os.path.exists(os.path.join(archive_dir, 'train', f'{x}.jpg')))]\n",
    "imagens_ausentes_test = archive_test_metadata[~archive_test_metadata['image_name'].apply(lambda x: os.path.exists(os.path.join(archive_dir, 'test', f'{x}.jpg')))]\n",
    "if len(imagens_ausentes_train) > 0:\n",
    "    print(f\"Imagens ausentes no dataset Kaggle (train): {len(imagens_ausentes_train)}\")\n",
    "if len(imagens_ausentes_test) > 0:\n",
    "    print(f\"Imagens ausentes no dataset Kaggle (test): {len(imagens_ausentes_test)}\")\n",
    "\n",
    "# Filtrar imagens existentes\n",
    "archive_train_metadata = archive_train_metadata[archive_train_metadata['image_name'].apply(lambda x: os.path.exists(os.path.join(archive_dir, 'train', f'{x}.jpg')))]\n",
    "archive_test_metadata = archive_test_metadata[archive_test_metadata['image_name'].apply(lambda x: os.path.exists(os.path.join(archive_dir, 'test', f'{x}.jpg')))]\n",
    "\n",
    "# Copiar imagens do treino\n",
    "for image_name in archive_train_metadata['image_name']:\n",
    "    src_path = os.path.join(archive_dir, 'train', f'{image_name}.jpg')\n",
    "    dest_path = os.path.join(image_output_dir, f'{image_name}.jpg')\n",
    "    shutil.copy(src_path, dest_path)\n",
    "\n",
    "# Copiar imagens do teste\n",
    "for image_name in archive_test_metadata['image_name']:\n",
    "    src_path = os.path.join(archive_dir, 'test', f'{image_name}.jpg')\n",
    "    dest_path = os.path.join(image_output_dir, f'{image_name}.jpg')\n",
    "    shutil.copy(src_path, dest_path)\n",
    "\n",
    "archive_train_metadata['image_path'] = archive_train_metadata['image_name'].apply(lambda x: os.path.join(image_output_dir, f'{x}.jpg'))\n",
    "archive_test_metadata['image_path'] = archive_test_metadata['image_name'].apply(lambda x: os.path.join(image_output_dir, f'{x}.jpg'))\n",
    "\n",
    "# Combinar os metadados\n",
    "for col in set(isic_metadata.columns) - set(common_columns):\n",
    "    archive_test_metadata[col] = pd.NA\n",
    "\n",
    "combined_metadata = pd.concat([isic_metadata, archive_train_metadata, archive_test_metadata], ignore_index=True)\n",
    "\n",
    "# Tratar imagens duplicadas\n",
    "duplicatas = combined_metadata.duplicated(subset='image_path', keep=False)\n",
    "imagens_duplicadas = combined_metadata[duplicatas]\n",
    "if len(imagens_duplicadas) > 0:\n",
    "    print(f\"Imagens duplicadas encontradas: {len(imagens_duplicadas)}\")\n",
    "    # Mantemos apenas a primeira ocorrência de cada imagem duplicada\n",
    "    combined_metadata = combined_metadata.drop_duplicates(subset='image_path', keep='first')\n",
    "\n",
    "# Salvar os metadados combinados\n",
    "combined_metadata_path = os.path.join(output_dir, 'metadata.csv')\n",
    "combined_metadata.to_csv(combined_metadata_path, index=False)\n",
    "\n",
    "total_imagens = len(combined_metadata)\n",
    "print(f\"Total de imagens no novo dataset: {total_imagens}\")\n",
    "\n",
    "print(\"Unificação completa. Os arquivos estão salvos em:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d9d5a670-e874-4044-9689-b6ae5a95e6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "#Redimensiona as imagens para ficar com tamanhos iguais\n",
    "\n",
    "def resize_images_in_directory(directory, size=(512, 512)):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):\n",
    "            image_path = os.path.join(directory, filename)\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.resize(size, Image.LANCZOS)\n",
    "                img.save(image_path)\n",
    "            \n",
    "resize_images_in_directory('/home/abel/Downloads/TCC-PUC-Melanoma-Detection/dataset/melanoma')\n",
    "resize_images_in_directory('/home/abel/Downloads/TCC-PUC-Melanoma-Detection/dataset/not_melanoma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c669820-edeb-499a-b8ff-7ec8c6361363",
   "metadata": {},
   "source": [
    "Preparando o dataset para treinamento, validação e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cbe22bad-c7dc-459a-a01d-e69b51d48e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagens organizadas em pastas para treino, validação e teste!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Diretório do dataset\n",
    "base_dir = '/home/abel/Downloads/TCC-PUC-Melanoma-Detection/dataset'\n",
    "\n",
    "melanoma_dir = os.path.join(base_dir, 'melanoma')\n",
    "not_melanoma_dir = os.path.join(base_dir, 'not_melanoma')\n",
    "\n",
    "# Criar diretórios de train, val, test\n",
    "os.makedirs(os.path.join(base_dir, 'train', 'melanoma'), exist_ok=True)\n",
    "os.makedirs(os.path.join(base_dir, 'train', 'not_melanoma'), exist_ok=True)\n",
    "os.makedirs(os.path.join(base_dir, 'val', 'melanoma'), exist_ok=True)\n",
    "os.makedirs(os.path.join(base_dir, 'val', 'not_melanoma'), exist_ok=True)\n",
    "os.makedirs(os.path.join(base_dir, 'test', 'melanoma'), exist_ok=True)\n",
    "os.makedirs(os.path.join(base_dir, 'test', 'not_melanoma'), exist_ok=True)\n",
    "\n",
    "# Mover os arquivos\n",
    "def move_files(file_list, source_dir, target_dir):\n",
    "    for file_name in file_list:\n",
    "        shutil.move(os.path.join(source_dir, file_name), os.path.join(target_dir, file_name))\n",
    "\n",
    "melanoma_files = os.listdir(melanoma_dir)\n",
    "not_melanoma_files = os.listdir(not_melanoma_dir)\n",
    "\n",
    "# Dividir os arquivos em train, val, test (80%, 10%, 10%)\n",
    "melanoma_train, melanoma_temp = train_test_split(melanoma_files, test_size=0.2, random_state=42)\n",
    "melanoma_val, melanoma_test = train_test_split(melanoma_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "not_melanoma_train, not_melanoma_temp = train_test_split(not_melanoma_files, test_size=0.2, random_state=42)\n",
    "not_melanoma_val, not_melanoma_test = train_test_split(not_melanoma_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "move_files(melanoma_train, melanoma_dir, os.path.join(base_dir, 'train', 'melanoma'))\n",
    "move_files(melanoma_val, melanoma_dir, os.path.join(base_dir, 'val', 'melanoma'))\n",
    "move_files(melanoma_test, melanoma_dir, os.path.join(base_dir, 'test', 'melanoma'))\n",
    "\n",
    "move_files(not_melanoma_train, not_melanoma_dir, os.path.join(base_dir, 'train', 'not_melanoma'))\n",
    "move_files(not_melanoma_val, not_melanoma_dir, os.path.join(base_dir, 'val', 'not_melanoma'))\n",
    "move_files(not_melanoma_test, not_melanoma_dir, os.path.join(base_dir, 'test', 'not_melanoma'))\n",
    "\n",
    "print(\"Imagens organizadas em pastas para treino, validação e teste!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
